{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Spark accumulators and visualization tools.\n",
    "\n",
    "### Part 1: Spark SQL - Joins\n",
    "\n",
    "During the exercises, the following resources might come in handy:\n",
    "* The [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)\n",
    "* The [Python documentation](https://docs.python.org/3.7/)\n",
    "* The [Spark SQL API documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "To run code in Jupyter, press: \n",
    "* `Ctrl-Enter` to run the code in the currently selected cell\n",
    "* `Shift-Enter` to run the code in the currently selected cell and jump to the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to execute these lines in a python script, you will need to create first a spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"SPARK_OPTS\"] = \"--driver-java-options=-Xms1024M --driver-java-options=-Xmx1536M --driver-java-options=-Dlog4j.logLevel=info\"\n",
    "\n",
    "# from pyspark import SparkContext, StorageLevel\n",
    "# from pyspark.sql import SQLContext\n",
    "\n",
    "# sc = SparkContext(master=\"local[*]\")\n",
    "# sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we are using the notebooks, those lines are not needed here.\n",
    "\n",
    "The following code creates the `displayRows()` function again, like we saw in Lab 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "def displayRows(rowDf):\n",
    "    headers = []\n",
    "    rows = []\n",
    "    if(str(type(rowDf)) == \"<class 'pyspark.sql.dataframe.DataFrame'>\"):\n",
    "        rows = rowDf.limit(10000).collect() #Let's limit the output just in case!\n",
    "        if(len(rows) == 10000):\n",
    "            if(rowDf.limit(10001).count() == 10001):\n",
    "                warnings.warn(\"More than 10 000 rows was returned, only showing the first 10 000.\")\n",
    "                \n",
    "        headers = list(rowDf.columns)\n",
    "    else:\n",
    "        rows = rowDf\n",
    "        if(len(rows) > 10000):\n",
    "            warnings.warn(\"Rows has {0} elements, only showing the first 10 000.\".format(len(rows)))\n",
    "            rows = rows[0:10000]\n",
    "            \n",
    "        #Computes the unique set of keys\n",
    "        headers = list(sorted(reduce(lambda x,y: x.union(set(y.asDict().iterkeys())), rows, set())))\n",
    "            \n",
    "    tableHead = [\"<th>{0}</th>\".format(key) for key in headers]\n",
    "    tableBody = [\"<tr>{0}</tr>\".format(\n",
    "                    \"\".join([\"<td>{0}</td>\".format(rowDict.get(header)) \n",
    "                            for rowDict \n",
    "                            in (row.asDict(),) \n",
    "                            for header \n",
    "                            in headers])\n",
    "                    ) for row in rows]\n",
    "    \n",
    "    display(HTML(\n",
    "    u\"\"\"<table>\n",
    "    <thead><tr>{0}</tr></thead>\n",
    "    <tbody>{1}</tbody>\n",
    "    </table>\n",
    "    \"\"\".format(\"\".join(tableHead), \"\".join(tableBody))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Spark SQL - Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part will introduce Spark SQL.\n",
    "\n",
    "The cell below generates the data which you will write queries for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 boy and girl names 2014 in random order.\n",
    "names = [\"Caden\", \"Kaylee\", \"Lucas\", \"Ethan\", \"Alexander\", \"Jackson\", \n",
    "         \"Aiden\", \"Madelyn\", \"Michael\", \"Avery\", \"Luke\", \"Isabella\", \n",
    "         \"Chloe\", \"Elijah\", \"Abigail\", \"Madison\", \"Jacob\", \"Zoe\", \"Emily\", \n",
    "         \"Jayden\", \"Liam\", \"Mason\", \"Mia\", \"Sophia\", \"Benjamin\", \"Layla\", \n",
    "         \"Emma\", \"Lily\", \"Charlotte\", \"Caleb\", \"James\", \"Noah\", \"Ella\", \n",
    "         \"Jack\", \"Jayce\", \"Aubrey\", \"Olivia\", \"Harper\", \"Logan\", \"Ava\"]\n",
    "\n",
    "#A-G in phonetic alphabet\n",
    "groups = [\"Alpha\",\"Bravo\", \"Charlie\", \"Delta\", \"Echo\", \"Foxtrot\", \"Golf\"]\n",
    "\n",
    "#Some numeric magic to generate not so uniform random data.\n",
    "tblUserRdd = sc.parallelize(map(lambda i: (i, ((i*104729)^131) % 7, 26500 + ((i*104729)^96587) % 6367), range(1,51)))\n",
    "tblNamesRdd = sc.parallelize(enumerate(names, 1), 4)\n",
    "tblGroupNamesRdd = sc.parallelize(enumerate(groups), 2)\n",
    "\n",
    "#Create dataframes from the RDDs\n",
    "tblNames      = sqlContext.createDataFrame(tblNamesRdd,      [\"userId\", \"name\"])\n",
    "tblUsers      = sqlContext.createDataFrame(tblUserRdd,       [\"id\", \"groupId\", \"salary\"])\n",
    "tblGroupNames = sqlContext.createDataFrame(tblGroupNamesRdd, [\"id\", \"name\"])\n",
    "\n",
    "#Register them for use.\n",
    "sqlContext.registerDataFrameAsTable(tblGroupNames, \"tblGroupNames\")\n",
    "sqlContext.registerDataFrameAsTable(tblUsers, \"tblUsers\")\n",
    "sqlContext.registerDataFrameAsTable(tblNames, \"tblNames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next 3 cells will display the content of the dataframe by using the helper function `displayRows()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayRows(tblUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayRows(tblNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayRows(tblGroupNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a) Merging user and group names of the users\n",
    "\n",
    "You will be using 2 inner joins to join `tblNames` and `tblGroupNames`\n",
    "\n",
    "Some names will be lost due to the inner join, but we will find them later.\n",
    "\n",
    "The result should have the following columns:\n",
    "\n",
    "1. **id**: User id\n",
    "2. **name**: The user name\n",
    "3. **groupName**: The group name\n",
    "\n",
    "Sort by **name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1a = sqlContext.sql(\"\"\"\n",
    "    SELECT \n",
    "        tblUsers.id, \n",
    "        tblNames.name AS name,\n",
    "        tblGroupNames.name AS groupName\n",
    "    FROM tblUsers \n",
    "    INNER JOIN tblNames ON tblUsers.id=tblNames.userId\n",
    "    INNER JOIN tblGroupNames ON tblUsers.groupId=tblGroupNames.id\n",
    "    order by name\n",
    "\"\"\")\n",
    "\n",
    "displayRows(q1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1aResult = q1a.collect()\n",
    "assert len(q1aResult) == 40\n",
    "assert all(map(lambda i: q1aResult[i].name <= q1aResult[i+1].name, range(0,39)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b) Find only the users that do not have any name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using either a left outer join or a left join between `tblUsers` and `tblNames`, we can find the users which do not have any name by checking if the name ```IS NULL```.\n",
    "\n",
    "Without this check we would get all users and ```NULL``` in the name column for those users which have no name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1b = sqlContext.sql(\"\"\"\n",
    "    SELECT \n",
    "        tblUsers.id, \n",
    "        tblGroupNames.name AS groupName,\n",
    "        tblNames.name\n",
    "    FROM tblUsers \n",
    "    LEFT JOIN tblNames ON tblUsers.id=tblNames.userId\n",
    "    INNER JOIN tblGroupNames ON tblUsers.groupId = tblGroupNames.id\n",
    "    where tblNames.name is Null\n",
    "\"\"\")\n",
    "\n",
    "displayRows(q1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1bresult = q1b.collect()\n",
    "assert len(q1bresult) == 10\n",
    "assert set(map(lambda row: row.id, q1bresult)) == set(range(41,51))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c) Counting name initials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often useful to perform queries on a result, or perform operations on columns on data which forces you to split up the query into multiple parts.\n",
    "\n",
    "In this exercise you will use a powerful mechanism that allows you to query the result of a subquery by treating the result as a dataframe. We have provided the subquery which uses a string function SUBSTR that extracts the first character of a string.\n",
    "\n",
    "The result should have the following columns:\n",
    "\n",
    "1. Initial: The extracted Initial\n",
    "2. Counts: The number of names which starts with *Initial*\n",
    "\n",
    "Order by the computed result `Counts` descending, and then by initial; return the top 10 results.\n",
    "\n",
    "**Hint:** Use `COUNT`, `GROUP BY`, `ORDER BY`, `LIMIT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1c = sqlContext.sql(\"\"\"\n",
    "SELECT \n",
    "    Initial, \n",
    "    count(tblInitials.Initial) as Counts\n",
    "FROM\n",
    "(\n",
    "    SELECT SUBSTR(name,0,1) AS Initial FROM tblNames\n",
    ") AS tblInitials\n",
    "group by Initial\n",
    "order by Counts desc, Initial\n",
    "limit 10\n",
    "\"\"\")\n",
    "                     \n",
    "displayRows(q1c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1cresult = q1c.collect()\n",
    "assert len(q1cresult) == 10\n",
    "assert q1cresult[0].Initial == \"A\" and q1cresult[1].Initial == \"J\" and q1cresult[2].Initial == \"L\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
