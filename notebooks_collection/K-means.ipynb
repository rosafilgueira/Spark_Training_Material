{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the dataset. It will have 1500 samples and we want to use k-means to detect clusters in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 1500\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the dataset. The clusters are clearly visible to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the k-means calculation we pick three centroids - randomly, so in this case we select the first three points in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "centroids = np.array(X[:k])\n",
    "print(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the dataset again and show the locations of the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1])\n",
    "plt.show()\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start using Spark. The first operation is the creation of a \"Resilient Distributed Dataset\" (RDD) which means distributing the data to the available nodes. This could be any size of underlying cluster - the user doesn't see the details.\n",
    "\n",
    "Note that this is just a demo and normally the dataset would be available on the compute nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(X)\n",
    "rdd.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tell Spark to cache this dataset because k-means is an iterative algorithm and we will scan through the data many times. (In this case, the dataset is so small that it will be easily kept in memory. However, if necessary, Spark can spill to disk if the dataset grows too large.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many records are in the dataset: Call the function count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the k-means algorithm we need to define a cost function. We will use the squared Euclidean distance which is the \"distance\" between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def cost_function(x, y):\n",
    "    return sqrt((x[0]-y[0])**2 + (x[1]-y[1])**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the cost function on two random points of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function(X[205], X[978])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the first k-means iteration: For each point in the dataset we calculate the distances to each of the centroids. Then we assign the point to the centroid to which it has the smallest distance.\n",
    "\n",
    "Spark allows us to do this quite easily: \"For each\" means we apply the \"map\" function, and define the output as a lambda. So the following means that each point x is mapped to a tuple with itself (x) as the first element, and the second element is a list of distances to all the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = rdd.map(lambda x: (x, [cost_function(x, centroids[i]) for i in range(k)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, Spark has not actually \"materialised\" the dataset so no computations have actually happened because we only applied a \"transformation\". To kick off the calculation we need to use an \"action\", for example \"take\" which returns the first n elements of a dataset. Let's have a look at the first 2 entries that from our mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments = d.map(lambda x: (x[1].index(min(x[1])), x[0] ))\n",
    "#assignments = d.map(lambda x, c: (c.index(min(c)), x))\n",
    "assignments.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = ['b','g','y']\n",
    "\n",
    "#for i in range(k):\n",
    "#    all_assigned = np.array(assignments.filter(lambda (a, x): a==i).map(lambda (c, x): x).collect())\n",
    "#    plt.scatter(all_assigned[:,0], all_assigned[:,1], color=colours[i])\n",
    "#    plt.scatter([centroids[i][0]], [centroids[i][1]], color=['r'])\n",
    "\n",
    "\n",
    "for i in range(k):\n",
    "    all_assigned = np.array(assignments.filter(lambda a: a[0]==i).map(lambda x: x[1]).collect())\n",
    "    plt.scatter(all_assigned[:,0], all_assigned[:,1], color=colours[i])\n",
    "    plt.scatter([centroids[i][0]], [centroids[i][1]], color=['r'])\n",
    "plt.show()\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the assignments of points to their centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know all the distances we can pick the centroid that have the smallest distance to our point. To do this we map each point to a tuple of itself and the centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look very good. Both of the lower clusters are assigned to the same centroid and the one at the top is split across all three centroids. Let's see how many points are assigned to each cluster: This is a class \"Map-Reduce\" operation for counting groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assignments.map(lambda (c, a): (c, 1)).reduceByKey(lambda a,b: a+b).collect()\n",
    "assignments.map(lambda c: (c[0], 1)).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, Spark has a built-in function that does this directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the next iteration: For each cluster we calculate a new centroid as the mean of all points in this cluster. So we first need to separate the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = assignments.filter(lambda a: a[0]==0).map(lambda a: a[1])\n",
    "c1 = assignments.filter(lambda a: a[0]==1).map(lambda a: a[1])\n",
    "c2 = assignments.filter(lambda a: a[0]==2).map(lambda a: a[1])\n",
    "c0.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the mean for each cluster and that yields the new centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c0.sum())\n",
    "print(c0.count())\n",
    "new_centroids = [c0.sum()/c0.count(), c1.sum()/c1.count(), c2.sum()/c2.count()]\n",
    "print(new_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the same method as before to assign the points to their new centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = rdd.map(lambda x: (x, [cost_function(x, new_centroids[i]) for i in range(k)]))\n",
    "#new_assignments = d.map(lambda (x, c): (c.index(min(c)), x))\n",
    "new_assignments = d.map(lambda c: (c[1].index(min(c[1])), c[0]))\n",
    "new_assignments.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = ['b','g','y']\n",
    "for i in range(k):\n",
    "    #all_assigned = np.array(new_assignments.filter(lambda (a, x): a==i).map(lambda (c, x): x).collect())\n",
    "    all_assigned = np.array(new_assignments.filter(lambda a: a[0]==i).map(lambda x: x[1]).collect())\n",
    "    plt.scatter(all_assigned[:,0], all_assigned[:,1], color=colours[i])\n",
    "    plt.scatter([new_centroids[i][0]], [new_centroids[i][1]], color=['r'])\n",
    "plt.show()\n",
    "plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make this more efficient: We create functions for assigning points to centroids, and for recalculating the centroids as the mean of the points in each cluster. The combination of those functions gives us an iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(rdd, centroids):\n",
    "    d = rdd.map(lambda x: (x, [cost_function(x, centroids[i]) for i in range(k)]))\n",
    "    #return d.map(lambda (x, c): (c.index(min(c)), x))\n",
    "    return d.map(lambda c: (c[1].index(min(c[1])), c[0]))\n",
    "    \n",
    "def recalculate_centroids(assignments):\n",
    "    new_centroids = []\n",
    "    for i in range(k):\n",
    "        #a = assignments.filter(lambda (a, x): a==i).map(lambda (a, x): x)\n",
    "        a = assignments.filter(lambda a: a[0]==i).map(lambda x: x[1])\n",
    "        c = a.sum()/a.count()\n",
    "        new_centroids.append(c)\n",
    "    return new_centroids\n",
    "\n",
    "def iteration(rdd, centroids):\n",
    "    assigned = assign(rdd, centroids)\n",
    "    new_centroids = recalculate_centroids(assigned)\n",
    "    return assigned, new_centroids\n",
    "\n",
    "assigned, new_centroids = iteration(rdd, new_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visually check the results let's define a function for plotting the current cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_clusters(assignments, centroids):\n",
    "    colours = ['b','g','y']\n",
    "    for i in range(k):\n",
    "        #all_assigned = np.array(assignments.filter(lambda (a, x): a==i).map(lambda (c, x): x).collect())\n",
    "        all_assigned = np.array(assignments.filter(lambda a: a[0]==i).map(lambda x: x[1]).collect())\n",
    "        plt.scatter(all_assigned[:,0], all_assigned[:,1], color=colours[i])\n",
    "        plt.scatter([centroids[i][0]], [centroids[i][1]], color=['r'])\n",
    "    plt.show()\n",
    "    plt.gcf().clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_clusters(assigned, new_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now repeat the following iteration as often as you like... You will see how the centroids move and the algorithm  slowly finds the correct clustering. Eventually, when there is no reassignment of points any more, the algorithm stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned, new_centroids = iteration(rdd, new_centroids)\n",
    "draw_clusters(assigned, new_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
